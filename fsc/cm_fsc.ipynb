{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "cJVqXw3dRSX-"
      },
      "source": [
        "# Configuring the Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BTOyqWt3RXfU"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import librosa\n",
        "from datasets import load_dataset, Dataset\n",
        "from transformers import Wav2Vec2ForSequenceClassification, Wav2Vec2FeatureExtractor\n",
        "\n",
        "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, roc_auc_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np \n",
        "import pandas as pd\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import divexplorer \n",
        "import pandas as pd\n",
        "pd.set_option('max_colwidth', None)\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "from utils_analysis import filter_itemset_df_by_attributes, slice_by_itemset\n",
        "\n",
        "from divexplorer.FP_DivergenceExplorer import FP_DivergenceExplorer\n",
        "from divexplorer.FP_Divergence import FP_Divergence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FTbn0utIRbgS"
      },
      "outputs": [],
      "source": [
        "## Set device\n",
        "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "SEED = 42\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "np.random.seed(SEED)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "R2gPZZs-SaDr"
      },
      "source": [
        "# FSC Dataset - Inference and Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def map_to_array(example, audio_col = 'path'):\n",
        "    speech, _ = librosa.load(example[audio_col], sr=16000, mono=True)\n",
        "    example[\"speech\"] = speech\n",
        "    return example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def preprocess_function(examples):\n",
        "    inputs = feature_extractor(\n",
        "      examples,\n",
        "      sampling_rate=feature_extractor.sampling_rate, \n",
        "      padding=True, \n",
        "      return_tensors=\"pt\")\n",
        "    return inputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Load model\n",
        "model = Wav2Vec2ForSequenceClassification.from_pretrained(\"superb/wav2vec2-base-superb-ic\").to(device)\n",
        "feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(\"superb/wav2vec2-base-superb-ic\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train and Valid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eCSRYY5nSemQ"
      },
      "outputs": [],
      "source": [
        "## Load and preprocess dataset\n",
        "df_train = pd.read_csv('data/train_data.csv')\n",
        "df_valid = pd.read_csv('data/valid_data.csv')\n",
        "df = pd.concat([df_train, df_valid], ignore_index=True)\n",
        "len(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset = Dataset.from_pandas(df)\n",
        "dataset = dataset.map(lambda x: map_to_array(x, audio_col='path'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nmQkoBnuSkMM"
      },
      "outputs": [],
      "source": [
        "## Inference\n",
        "hidden_states_concatenation = []\n",
        "logits_concatenation = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for i in tqdm(range(0, len(dataset))):\n",
        "        inputs = preprocess_function(dataset[i][\"speech\"]).to(device)\n",
        "        outputs = model(**inputs)\n",
        "        hidden_states_concatenation.append(outputs.hidden_states[-1])\n",
        "        logits_concatenation.append(outputs.logits)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.save(hidden_states_concatenation, 'pretrained/hidden_states.pt')\n",
        "torch.save(logits_concatenation, 'pretrained/logits.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Load hidden states and logits\n",
        "hidden_states_concatenation = torch.load('pretrained/hidden_states.pt')\n",
        "logits_concatenation = torch.load('pretrained/logits.pt')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "P_SSWxldSmc-"
      },
      "source": [
        "### Intent Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JjUcqGR-SpVq",
        "outputId": "50e6bdd6-cd77-48e4-8883-e57750d46f93"
      },
      "outputs": [],
      "source": [
        "action_ids = []\n",
        "for i in range(len(logits_concatenation)):\n",
        "    logits = logits_concatenation[i].detach().cpu()\n",
        "    action_ids.append(torch.argmax(logits[:, :6], dim=-1).item())\n",
        "action_labels = [model.config.id2label[_id] for _id in action_ids]\n",
        "\n",
        "action_gt = list(df['action'].values)\n",
        "\n",
        "print(\"Action accuracy: \", round(accuracy_score(action_gt, action_labels)*100, 2), \"%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6T3Otk-TSqp0",
        "outputId": "79bc9ef2-56fa-4779-a79f-4b6fd8c2f840"
      },
      "outputs": [],
      "source": [
        "object_ids = []\n",
        "for i in range(len(logits_concatenation)):\n",
        "    logits = logits_concatenation[i].detach().cpu()\n",
        "    object_ids.append(torch.argmax(logits[:, 6:20], dim=-1).item())\n",
        "object_labels = [model.config.id2label[_id + 6] for _id in object_ids]\n",
        "\n",
        "object_gt = list(df['object'].values)\n",
        "object_gt = [f'{x}_object' if x=='none' else x for x in object_gt]\n",
        "\n",
        "print(\"Obejct accuracy: \", round(accuracy_score(object_gt, object_labels)*100, 2), \"%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2os4Ue9iSr4L",
        "outputId": "e1454b83-6a71-4583-a177-02a1d16b32f5"
      },
      "outputs": [],
      "source": [
        "location_ids = []\n",
        "for i in range(len(logits_concatenation)):\n",
        "    logits = logits_concatenation[i].detach().cpu()\n",
        "    location_ids.append(torch.argmax(logits[:, 20:24], dim=-1).item())\n",
        "location_labels = [model.config.id2label[_id + 20] for _id in location_ids]\n",
        "\n",
        "location_gt = list(df['location'].values)\n",
        "location_gt = [f'{x}_location' if x=='none' else x for x in location_gt]\n",
        "\n",
        "print(\"Location accuracy: \", round(accuracy_score(location_gt, location_labels)*100, 2), \"%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8E7l-8ZYSzDN"
      },
      "outputs": [],
      "source": [
        "## Save predictions\n",
        "intents_predicted = [ action_labels[i]  + \" \" + object_labels[i] + \" \" + location_labels[i] for i in range(0, len(df))]\n",
        "intents_gt = [ action_gt[i]  + \" \" + object_gt[i] + \" \" + location_gt[i] for i in range(0, len(df))]\n",
        "\n",
        "is_correct = (np.array(intents_predicted) == np.array(intents_gt)).astype(int)\n",
        "df['prediction'] = is_correct\n",
        "print(\"Accuracy: \", round(np.mean(is_correct)*100,2), \"%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Save hidden states\n",
        "df['hidden_states'] = [hs.detach().cpu().numpy().squeeze() for hs in hidden_states_concatenation]\n",
        "df['hidden_states'] = df['hidden_states'].apply(lambda x: x.astype(float))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Save action, object and location predictions \n",
        "df['predicted_action'] = [l[:, :6].detach().cpu().numpy().squeeze() for l in logits_concatenation]\n",
        "df['predicted_object'] = [l[:, 6:20].detach().cpu().numpy().squeeze() for l in logits_concatenation]\n",
        "df['predicted_location'] = [l[:, 20:24].detach().cpu().numpy().squeeze() for l in logits_concatenation]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "output_folder = os.path.join(f'fsc_train_valid.csv')\n",
        "df.to_csv(output_folder, index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Load and preprocess dataset\n",
        "df_test = pd.read_csv('data/test_data.csv')\n",
        "len(df_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset_test = Dataset.from_pandas(df_test) \n",
        "dataset_test = dataset_test.map(lambda x: map_to_array(x, audio_col='path'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Inference\n",
        "hidden_states_concatenation_test = []\n",
        "logits_concatenation_test = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for i in tqdm(range(0, len(dataset_test))):\n",
        "        inputs = preprocess_function(dataset_test[i][\"speech\"]).to(device)\n",
        "        outputs = model(**inputs)\n",
        "        hidden_states_concatenation_test.append(outputs.hidden_states[-1])\n",
        "        logits_concatenation_test.append(outputs.logits)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.save(hidden_states_concatenation_test, 'pretrained/hidden_states_test.pt')\n",
        "torch.save(logits_concatenation_test, 'pretrained/logits_test.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Load hidden states and logits\n",
        "hidden_states_concatenation_test = torch.load('pretrained/hidden_states_test.pt')\n",
        "logits_concatenation_test = torch.load('pretrained/logits_test.pt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Intent Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "action_ids = []\n",
        "for i in range(len(logits_concatenation_test)):\n",
        "    logits = logits_concatenation_test[i].detach().cpu()\n",
        "    action_ids.append(torch.argmax(logits[:, :6], dim=-1).item())\n",
        "action_labels = [model.config.id2label[_id] for _id in action_ids]\n",
        "\n",
        "action_gt = list(df_test['action'].values)\n",
        "\n",
        "print(\"Action accuracy: \", round(accuracy_score(action_gt, action_labels)*100, 2), \"%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "object_ids = []\n",
        "for i in range(len(logits_concatenation_test)):\n",
        "    logits = logits_concatenation_test[i].detach().cpu()\n",
        "    object_ids.append(torch.argmax(logits[:, 6:20], dim=-1).item())\n",
        "object_labels = [model.config.id2label[_id + 6] for _id in object_ids]\n",
        "\n",
        "object_gt = list(df_test['object'].values)\n",
        "object_gt = [f'{x}_object' if x=='none' else x for x in object_gt]\n",
        "\n",
        "print(\"Obejct accuracy: \", round(accuracy_score(object_gt, object_labels)*100, 2), \"%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "location_ids = []\n",
        "for i in range(len(logits_concatenation_test)):\n",
        "    logits = logits_concatenation_test[i].detach().cpu()\n",
        "    location_ids.append(torch.argmax(logits[:, 20:24], dim=-1).item())\n",
        "location_labels = [model.config.id2label[_id + 20] for _id in location_ids]\n",
        "\n",
        "location_gt = list(df_test['location'].values)\n",
        "location_gt = [f'{x}_location' if x=='none' else x for x in location_gt]\n",
        "\n",
        "print(\"Location accuracy: \", round(accuracy_score(location_gt, location_labels)*100, 2), \"%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Save predictions\n",
        "intents_predicted = [ action_labels[i]  + \" \" + object_labels[i] + \" \" + location_labels[i] for i in range(0, len(df_test))]\n",
        "intents_gt = [ action_gt[i]  + \" \" + object_gt[i] + \" \" + location_gt[i] for i in range(0, len(df_test))]\n",
        "\n",
        "is_correct = (np.array(intents_predicted) == np.array(intents_gt)).astype(int)\n",
        "df_test['prediction'] = is_correct\n",
        "print(\"Accuracy: \", round(np.mean(is_correct)*100,2), \"%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Save hidden states\n",
        "df_test['hidden_states'] = [hs.detach().cpu().numpy().squeeze() for hs in hidden_states_concatenation_test]\n",
        "df_test['hidden_states'] = df_test['hidden_states'].apply(lambda x: x.astype(float))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Save action, object and location predictions \n",
        "df_test['predicted_action'] = [l[:, :6].detach().cpu().numpy().squeeze() for l in logits_concatenation_test]\n",
        "df_test['predicted_object'] = [l[:, 6:20].detach().cpu().numpy().squeeze() for l in logits_concatenation_test]\n",
        "df_test['predicted_location'] = [l[:, 20:24].detach().cpu().numpy().squeeze() for l in logits_concatenation_test]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "output_folder = os.path.join(f'fsc_test.csv')\n",
        "df_test.to_csv(output_folder, index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Loading Pretrained Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Load model\n",
        "model = Wav2Vec2ForSequenceClassification.from_pretrained(\"superb/wav2vec2-base-superb-ic\").to(device)\n",
        "feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(\"superb/wav2vec2-base-superb-ic\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Load and preprocess dataset\n",
        "df_train = pd.read_csv('data/train_data.csv')\n",
        "df_valid = pd.read_csv('data/valid_data.csv')\n",
        "df = pd.concat([df_train, df_valid], ignore_index=True)\n",
        "\n",
        "## Load hidden states and logits\n",
        "hidden_states_concatenation = torch.load('pretrained/hidden_states.pt')\n",
        "logits_concatenation = torch.load('pretrained/logits.pt')\n",
        "\n",
        "action_ids = []\n",
        "for i in range(len(logits_concatenation)):\n",
        "    logits = logits_concatenation[i].detach().cpu()\n",
        "    action_ids.append(torch.argmax(logits[:, :6], dim=-1).item())\n",
        "action_labels = [model.config.id2label[_id] for _id in action_ids]\n",
        "action_gt = list(df['action'].values)\n",
        "\n",
        "object_ids = []\n",
        "for i in range(len(logits_concatenation)):\n",
        "    logits = logits_concatenation[i].detach().cpu()\n",
        "    object_ids.append(torch.argmax(logits[:, 6:20], dim=-1).item())\n",
        "object_labels = [model.config.id2label[_id + 6] for _id in object_ids]\n",
        "object_gt = list(df['object'].values)\n",
        "object_gt = [f'{x}_object' if x=='none' else x for x in object_gt]\n",
        "\n",
        "location_ids = []\n",
        "for i in range(len(logits_concatenation)):\n",
        "    logits = logits_concatenation[i].detach().cpu()\n",
        "    location_ids.append(torch.argmax(logits[:, 20:24], dim=-1).item())\n",
        "location_labels = [model.config.id2label[_id + 20] for _id in location_ids]\n",
        "location_gt = list(df['location'].values)\n",
        "location_gt = [f'{x}_location' if x=='none' else x for x in location_gt]\n",
        "\n",
        "## Save predictions\n",
        "intents_predicted = [ action_labels[i]  + \" \" + object_labels[i] + \" \" + location_labels[i] for i in range(0, len(df))]\n",
        "intents_gt = [ action_gt[i]  + \" \" + object_gt[i] + \" \" + location_gt[i] for i in range(0, len(df))]\n",
        "is_correct = (np.array(intents_predicted) == np.array(intents_gt)).astype(int)\n",
        "df['prediction'] = is_correct\n",
        "\n",
        "## Save hidden states\n",
        "df['hidden_states'] = [hs.detach().cpu().numpy().squeeze() for hs in hidden_states_concatenation]\n",
        "df['hidden_states'] = df['hidden_states'].apply(lambda x: x.astype(float))\n",
        "\n",
        "## Save action, object and location predictions \n",
        "df['predicted_action'] = [l[:, :6].detach().cpu().numpy().squeeze() for l in logits_concatenation]\n",
        "df['predicted_object'] = [l[:, 6:20].detach().cpu().numpy().squeeze() for l in logits_concatenation]\n",
        "df['predicted_location'] = [l[:, 20:24].detach().cpu().numpy().squeeze() for l in logits_concatenation]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Load and preprocess dataset\n",
        "df_test = pd.read_csv('data/test_data.csv')\n",
        "\n",
        "## Load hidden states and logits\n",
        "hidden_states_concatenation_test = torch.load('pretrained/hidden_states_test.pt')\n",
        "logits_concatenation_test = torch.load('pretrained/logits_test.pt')\n",
        "\n",
        "action_ids = []\n",
        "for i in range(len(logits_concatenation_test)):\n",
        "    logits = logits_concatenation_test[i].detach().cpu()\n",
        "    action_ids.append(torch.argmax(logits[:, :6], dim=-1).item())\n",
        "action_labels = [model.config.id2label[_id] for _id in action_ids]\n",
        "action_gt = list(df_test['action'].values)\n",
        "\n",
        "object_ids = []\n",
        "for i in range(len(logits_concatenation_test)):\n",
        "    logits = logits_concatenation_test[i].detach().cpu()\n",
        "    object_ids.append(torch.argmax(logits[:, 6:20], dim=-1).item())\n",
        "object_labels = [model.config.id2label[_id + 6] for _id in object_ids]\n",
        "object_gt = list(df_test['object'].values)\n",
        "object_gt = [f'{x}_object' if x=='none' else x for x in object_gt]\n",
        "\n",
        "location_ids = []\n",
        "for i in range(len(logits_concatenation_test)):\n",
        "    logits = logits_concatenation_test[i].detach().cpu()\n",
        "    location_ids.append(torch.argmax(logits[:, 20:24], dim=-1).item())\n",
        "location_labels = [model.config.id2label[_id + 20] for _id in location_ids]\n",
        "location_gt = list(df_test['location'].values)\n",
        "location_gt = [f'{x}_location' if x=='none' else x for x in location_gt]\n",
        "\n",
        "## Save predictions\n",
        "intents_predicted = [ action_labels[i]  + \" \" + object_labels[i] + \" \" + location_labels[i] for i in range(0, len(df_test))]\n",
        "intents_gt = [ action_gt[i]  + \" \" + object_gt[i] + \" \" + location_gt[i] for i in range(0, len(df_test))]\n",
        "is_correct = (np.array(intents_predicted) == np.array(intents_gt)).astype(int)\n",
        "df_test['prediction'] = is_correct\n",
        "\n",
        "## Save hidden states\n",
        "df_test['hidden_states'] = [hs.detach().cpu().numpy().squeeze() for hs in hidden_states_concatenation_test]\n",
        "df_test['hidden_states'] = df_test['hidden_states'].apply(lambda x: x.astype(float))\n",
        "\n",
        "## Save action, object and location predictions \n",
        "df_test['predicted_action'] = [l[:, :6].detach().cpu().numpy().squeeze() for l in logits_concatenation_test]\n",
        "df_test['predicted_object'] = [l[:, 6:20].detach().cpu().numpy().squeeze() for l in logits_concatenation_test]\n",
        "df_test['predicted_location'] = [l[:, 20:24].detach().cpu().numpy().squeeze() for l in logits_concatenation_test]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Confidence Model "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Confidence model\n",
        "class ConfidenceModel(nn.Module):\n",
        "    def __init__(self, input_size=768, hidden_size=1000, output_size=1):\n",
        "        super(ConfidenceModel, self).__init__()\n",
        "        self.linear1 = nn.Linear(input_size, hidden_size)\n",
        "        self.linear2 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.linear3 = nn.Linear(hidden_size, output_size)\n",
        "        self.relu = nn.GELU()\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.norm = nn.LayerNorm(hidden_size)\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')\n",
        "                nn.init.zeros_(m.bias)\n",
        "                                     \n",
        "    def forward(self,x):\n",
        "        x = self.relu(self.linear1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.norm(x)\n",
        "        x = self.relu(self.linear2(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.norm(x)\n",
        "        x = self.sigmoid(self.linear3(x))\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Train, valid and test\n",
        "def train(model, inputs, labels, criterion, optimizer):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(inputs.float())\n",
        "    loss = criterion(outputs, labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return outputs, loss.item()\n",
        "\n",
        "def val(model, inputs, labels, criterion):\n",
        "    model.eval()\n",
        "    outputs = model(inputs.float())\n",
        "    loss = criterion(outputs, labels)\n",
        "    return outputs, loss.item()\n",
        "\n",
        "def test(model, inputs, labels=None, criterion=None):\n",
        "    model.eval()\n",
        "    if labels is None and criterion is None:\n",
        "        outputs = model(inputs.float())\n",
        "        return outputs\n",
        "    else:\n",
        "        outputs = model(inputs.float())\n",
        "        loss = criterion(outputs, labels)\n",
        "        return outputs, loss.item()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Problem Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "HIDDEN_SIZE = 1000\n",
        "BATCH_SIZE = 4096\n",
        "NUM_SUBGROUPS = 2\n",
        "EPOCHS = 10000\n",
        "MIN_SUP = 0.03\n",
        "TH_REDUNDANCY = 0.05\n",
        "PRETRAIN = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Problematic Subgroups Identification - Step 2, DivExplorer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Define abbreviations for plot and visualization\n",
        "from divexplorer.FP_Divergence import abbreviateDict\n",
        "abbreviations = {'Self-reported fluency level=native': 'fluency=native', \\\n",
        "                  'total_silence':'tot_silence', 'location': 'loc', \\\n",
        "                  'Current language used for work/school=English (United States)': 'lang=EN_US', \\\n",
        "                  'speakerId' : 'spkID', \\\n",
        "                  'First Language spoken=English (United States)':  'lang=EN_US', \\\n",
        "                  'trimmed':'trim', \\\n",
        "                  'total_':'', \\\n",
        "                  'speed_rate_word':'speakRate', \\\n",
        "                  'speed_rate_char':'speakCharRate', \\\n",
        "                  'change language': 'change lang', \\\n",
        "                  'duration': 'dur'}\n",
        "\n",
        "abbreviations_shorter = abbreviations.copy()\n",
        "\n",
        "## Function for sorting data cohorts\n",
        "def sortItemset(x, abbreviations={}):\n",
        "    x = list(x)\n",
        "    x.sort()\n",
        "    x = \", \".join(x)\n",
        "    for k, v in abbreviations.items():\n",
        "        x = x.replace(k, v)\n",
        "    return x\n",
        "\n",
        "def attributes_in_itemset(itemset, attributes, alls = True):\n",
        "    \"\"\" Check if attributes are in the itemset (all or at least one)\n",
        "    \n",
        "    Args:\n",
        "        itemset (frozenset): the itemset\n",
        "        attributes (list): list of itemset of interest\n",
        "        alls (bool): If True, check if ALL attributes of the itemset are the input attributes. \n",
        "        If False, check AT LEAST one attribute of the itemset is in the input attributes.\n",
        "        \n",
        "    \"\"\"\n",
        "    # Avoid returning the empty itemset (i.e., info of entire dataset)\n",
        "    if itemset == frozenset() and attributes:\n",
        "        return False\n",
        "    \n",
        "    for item in itemset:\n",
        "        # Get the attribute\n",
        "        attr_i = item.split(\"=\")[0]\n",
        "        \n",
        "        #If True, check if ALL attributes of the itemset are the input attributes.\n",
        "        if alls:\n",
        "            # Check if the attribute is present. If not, the itemset is not admitted\n",
        "            if attr_i not in attributes:\n",
        "                return False\n",
        "        else:\n",
        "            # Check if least one attribute. If yes, return True\n",
        "            if attr_i in attributes:\n",
        "                return True\n",
        "    if alls:\n",
        "        # All attributes of the itemset are indeed admitted\n",
        "        return True\n",
        "    else:\n",
        "        # Otherwise, it means that we find None\n",
        "        return False\n",
        "    \n",
        "def filter_itemset_df_by_attributes(df: pd.DataFrame, attributes: list, alls = True, itemset_col_name: str = \"itemsets\") -> pd.DataFrame:\n",
        "    \"\"\"Get the set of itemsets that have the attributes in the input list (all or at least one)\n",
        "    \n",
        "    Args:\n",
        "        df (pd.DataFrame): the input itemsets (with their info). \n",
        "        attributes (list): list of itemset of interest\n",
        "        alls (bool): If True, check if ALL attributes of the itemset are the input attributes. \n",
        "        If False, check AT LEAST one attribute of the itemset is in the input attributes.\n",
        "        itemset_col_name (str) : the name of the itemset column, \"itemsets\" as default\n",
        "        \n",
        "    Returns:\n",
        "        pd.DataFrame: the set of itemsets (with their info)\n",
        "    \"\"\"\n",
        "\n",
        "    return df.loc[df[itemset_col_name].apply(lambda x: attributes_in_itemset(x, attributes, alls = alls))]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Target for DivExplorer: \n",
        "# 'prediction' is 1 if predicted_intet == original_intent, 0 otherwise\n",
        "target_col = 'prediction' \n",
        "target_metric = 'd_posr'\n",
        "target_div = 'd_accuracy'\n",
        "t_value_col = 't_value_tp_fn'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Columns for visualization\n",
        "show_cols = ['support', 'itemsets', '#errors', '#corrects', 'accuracy', \\\n",
        "                'd_accuracy', 't_value', 'support_count', 'length']\n",
        "remapped_cols = {'tn': '#errors', 'tp': '#corrects', 'posr': 'accuracy', \\\n",
        "                target_metric: target_div, 't_value_tp_fn': 't_value'}\n",
        "\n",
        "## Columns of the df file that we are going to analyze \n",
        "demo_cols = ['gender', 'ageRange']\n",
        "\n",
        "slot_cols = ['action', 'object', 'location']\n",
        "\n",
        "signal_cols = ['total_silence', 'total_duration', 'trimmed_duration', \n",
        "       'n_words', 'speed_rate_word', 'speed_rate_word_trimmed']      \n",
        " \n",
        "input_cols = demo_cols + signal_cols + slot_cols"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# select the columns of interest\n",
        "df_divexpl = df[[\n",
        "    'path', 'transcription', \n",
        "    'action', 'object', 'location', \n",
        "    'prediction', \n",
        "    'speakerId', 'gender', 'ageRange', 'Self-reported fluency level ', 'First Language spoken','Current language used for work/school',\n",
        "    'total_silence', 'total_duration', 'trimmed_duration', 'n_words', 'speed_rate_word', 'speed_rate_word_trimmed'\n",
        "    ]]\n",
        "\n",
        "df_test_divexpl = df_test[[\n",
        "    'path', 'transcription', \n",
        "    'action', 'object', 'location', \n",
        "    'prediction', \n",
        "    'speakerId', 'gender', 'ageRange', 'Self-reported fluency level ', 'First Language spoken','Current language used for work/school',\n",
        "    'total_silence', 'total_duration', 'trimmed_duration', 'n_words', 'speed_rate_word', 'speed_rate_word_trimmed'\n",
        "    ]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Add SpeakerID information if it is present in the df\n",
        "if \"speakerId\" in input_cols:\n",
        "    df_divexpl['speakerId'] = df_divexpl.index.map(lambda x: x.split(\"/\")[2])\n",
        "\n",
        "## Discretize the dataframe\n",
        "from util_discretization import discretize\n",
        "\n",
        "df_discretized = discretize(\n",
        "    df_divexpl[input_cols+[target_col]],\n",
        "    bins=3,\n",
        "    attributes=input_cols,\n",
        "    strategy=\"quantile\", \n",
        "    round_v = 2,\n",
        "    min_distinct=5,\n",
        ")\n",
        "\n",
        "## Replace values with ranges: \"low\", \"medium\", \"high\"\n",
        "replace_values = {}\n",
        "\n",
        "for i in range(0,len(signal_cols)):\n",
        "\n",
        "    for v in df_discretized[signal_cols[i]].unique():\n",
        "        if \"<=\" == v[0:2]:\n",
        "            replace_values[v] = \"low\"\n",
        "        elif \">\" == v[0]:\n",
        "            replace_values[v] = \"high\"\n",
        "        elif \"(\"  == v[0] and \"]\"  == v[-1]:\n",
        "            replace_values[v] = \"medium\"\n",
        "        else:\n",
        "            raise ValueError(v)\n",
        "\n",
        "    df_discretized[signal_cols[i]].replace(replace_values, inplace=True)\n",
        "    \n",
        "df_discretized.loc[df_discretized[\"location\"]==\"none_location\", \"location\"] = \"none\"\n",
        "df_discretized.loc[df_discretized[\"object\"]==\"none_object\", \"object\"] = \"none\"\n",
        "\n",
        "## Create dict of Divergence df\n",
        "fp_diver = FP_DivergenceExplorer(df_discretized, true_class_name=target_col, class_map={\"P\":1, \"N\":0})\n",
        "FP_fm = fp_diver.getFrequentPatternDivergence(min_support=MIN_SUP, metrics=[target_metric])\n",
        "FP_fm.rename(columns=remapped_cols, inplace=True)\n",
        "FP_fm = FP_fm[show_cols].copy()\n",
        "FP_fm['accuracy'] = round(FP_fm['accuracy'], 5)\n",
        "FP_fm['d_accuracy'] = round(FP_fm['d_accuracy'], 5)\n",
        "FP_fm['t_value'] = round(FP_fm['t_value'], 2)\n",
        "fp_divergence = FP_Divergence(FP_fm, target_div)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Compute the divergence for Wav2Vec2-Base\n",
        "FPdiv = fp_divergence.getDivergence(th_redundancy=TH_REDUNDANCY)[::-1]\n",
        "\n",
        "## Retrieve Most Divergent Itemsets \n",
        "from copy import deepcopy\n",
        "pr = FPdiv.copy()\n",
        "pr[\"support\"] = pr[\"support\"].round(2)\n",
        "pr[\"#errors\"] = pr[\"#errors\"].astype(int)\n",
        "pr[\"#corrects\"] = pr[\"#corrects\"].astype(int)\n",
        "pr[\"accuracy\"] = (pr[\"accuracy\"]*100).round(3)\n",
        "pr[\"d_accuracy\"] = (pr[\"d_accuracy\"]*100).round(3)\n",
        "pr.head(NUM_SUBGROUPS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Create a column in the df, and assign a class to each sample:\n",
        "# - 1 if the sample is in the most divergent itemset\n",
        "# - 2 if the sample is in the second most divergent itemset\n",
        "# - 3 if the sample is in the third most divergent itemset\n",
        "# - ...\n",
        "# - 0 otherwise\n",
        "\n",
        "df_discretized[\"subgID\"] = 0\n",
        "itemsets = []\n",
        "for i in range(NUM_SUBGROUPS):\n",
        "    itemsets.append(list(pr.itemsets.values[i]))\n",
        "for i in tqdm(range(0, len(df_discretized))):\n",
        "    for value,itemset in enumerate(itemsets):\n",
        "        ks = []\n",
        "        vs = []\n",
        "        for item in itemset:\n",
        "            k, v = item.split(\"=\")\n",
        "            ks.append(k)\n",
        "            vs.append(v)\n",
        "        if all(df_discretized.loc[i, ks] == vs):\n",
        "            if df_discretized.loc[i, \"subgID\"] == 0:\n",
        "                df_discretized.loc[i, \"subgID\"] = value+1\n",
        "            else:\n",
        "                continue\n",
        "        else:\n",
        "            continue\n",
        "if BINARY:\n",
        "    df_discretized.loc[df_discretized[\"subgID\"]!=0, \"subgID\"] = 1\n",
        "for i in range(0,NUM_SUBGROUPS+1):\n",
        "    print(len(df_discretized.loc[df_discretized[\"subgID\"]==i]))\n",
        "df_discretized.to_csv(\"df_discretized.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Add SpeakerID information if it is present in the df\n",
        "if \"speakerId\" in input_cols:\n",
        "    df_test_divexpl['speakerId'] = df_test_divexpl.index.map(lambda x: x.split(\"/\")[2])\n",
        "\n",
        "## Discretize the dataframe\n",
        "from util_discretization import discretize\n",
        "\n",
        "df_discretized_test = discretize(\n",
        "    df_test_divexpl[input_cols+[target_col]],\n",
        "    bins=3,\n",
        "    attributes=input_cols,\n",
        "    strategy=\"quantile\", \n",
        "    round_v = 2,\n",
        "    min_distinct=5,\n",
        ")\n",
        "\n",
        "## Replace values with ranges: \"low\", \"medium\", \"high\"\n",
        "replace_values = {}\n",
        "\n",
        "for i in range(0,len(signal_cols)):\n",
        "\n",
        "    for v in df_discretized_test[signal_cols[i]].unique():\n",
        "        if \"<=\" == v[0:2]:\n",
        "            replace_values[v] = \"low\"\n",
        "        elif \">\" == v[0]:\n",
        "            replace_values[v] = \"high\"\n",
        "        elif \"(\"  == v[0] and \"]\"  == v[-1]:\n",
        "            replace_values[v] = \"medium\"\n",
        "        else:\n",
        "            raise ValueError(v)\n",
        "\n",
        "    df_discretized_test[signal_cols[i]].replace(replace_values, inplace=True)\n",
        "    \n",
        "df_discretized_test.loc[df_discretized[\"location\"]==\"none_location\", \"location\"] = \"none\"\n",
        "df_discretized_test.loc[df_discretized[\"object\"]==\"none_object\", \"object\"] = \"none\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Create a column in the df, and assign a class to each sample:\n",
        "# - 1 if the sample is in the most divergent itemset\n",
        "# - 2 if the sample is in the second most divergent itemset\n",
        "# - 3 if the sample is in the third most divergent itemset\n",
        "# - ...\n",
        "# - 0 otherwise\n",
        "df_discretized_test[\"subgID\"] = 0\n",
        "for i in tqdm(range(0, len(df_discretized_test))):\n",
        "    for value,itemset in enumerate(itemsets):\n",
        "        ks = []\n",
        "        vs = []\n",
        "        for item in itemset:\n",
        "            k, v = item.split(\"=\")\n",
        "            ks.append(k)\n",
        "            vs.append(v)\n",
        "        if all(df_discretized_test.loc[i, ks] == vs):\n",
        "            if df_discretized_test.loc[i, \"subgID\"] == 0:\n",
        "                df_discretized_test.loc[i, \"subgID\"] = value+1\n",
        "            else:\n",
        "                continue\n",
        "        else:\n",
        "            continue\n",
        "if BINARY:\n",
        "    df_discretized_test.loc[df_discretized_test[\"subgID\"]!=0, \"subgID\"] = 1\n",
        "for i in range(0,NUM_SUBGROUPS+1):\n",
        "    print(len(df_discretized_test.loc[df_discretized_test[\"subgID\"]==i]))\n",
        "df_discretized_test.to_csv(\"df_discretized_test.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Full pipeline - Steps 1 & 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_cm = df[[\n",
        "    'prediction', \n",
        "    'predicted_action', 'predicted_object', 'predicted_location',\n",
        "    'hidden_states',\n",
        "    'total_silence', 'n_words', 'speed_rate_word'\n",
        "    ]]\n",
        "df_cm_test = df_test[[\n",
        "    'prediction', \n",
        "    'predicted_action', 'predicted_object', 'predicted_location',\n",
        "    'hidden_states',\n",
        "    'total_silence', 'n_words', 'speed_rate_word'\n",
        "    ]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Pretraining the CM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Create train, val, test split\n",
        "X = torch.cat((\n",
        "    torch.tensor(df_cm['predicted_action']),\n",
        "    torch.tensor(df_cm['predicted_object']),\n",
        "    torch.tensor(df_cm['predicted_location']),\n",
        "    torch.stack(\n",
        "        [torch.mean(torch.tensor(el[-1])) for el in df_cm['hidden_states']]\n",
        "        ).unsqueeze(1),\n",
        "    torch.tensor(df_cm['total_silence']).unsqueeze(1),\n",
        "    torch.tensor(df_cm['n_words']).unsqueeze(1),\n",
        "    torch.tensor(df_cm['speed_rate_word']).unsqueeze(1),\n",
        "    ), dim=1)\n",
        "y = torch.tensor(df_cm['prediction']).unsqueeze(1)\n",
        "X_train = X[:23132]\n",
        "y_train = y[:23132]\n",
        "X_val = X[23132:]\n",
        "y_val = y[23132:]\n",
        "\n",
        "X_test = torch.cat((\n",
        "    torch.tensor(df_cm_test['predicted_action']),\n",
        "    torch.tensor(df_cm_test['predicted_object']),\n",
        "    torch.tensor(df_cm_test['predicted_location']),\n",
        "    torch.stack(\n",
        "        [torch.mean(torch.tensor(el[-1])) for el in df_cm_test['hidden_states']]\n",
        "        ).unsqueeze(1),\n",
        "    torch.tensor(df_cm_test['total_silence']).unsqueeze(1),\n",
        "    torch.tensor(df_cm_test['n_words']).unsqueeze(1),\n",
        "    torch.tensor(df_cm_test['speed_rate_word']).unsqueeze(1),\n",
        "    ), dim=1)\n",
        "y_test = torch.tensor(df_cm_test['prediction']).unsqueeze(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "\n",
        "best_auc = 0\n",
        "best_acc = 0\n",
        "best_output = 0\n",
        "best_model = 0\n",
        "\n",
        "## Create model\n",
        "model = ConfidenceModel(\n",
        "    input_size=X_train.shape[1],\n",
        "    hidden_size=HIDDEN_SIZE, \n",
        "    output_size=1\n",
        "    ).to(device)        \n",
        "\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.NAdam(model.parameters(), lr=0.005)\n",
        "\n",
        "## Train model\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "test_losses = []\n",
        "test_aucs = []\n",
        "val_aucs = []\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "        \n",
        "    ## Train in batches\n",
        "    for i in range(0, len(X_train), BATCH_SIZE):\n",
        "        train_output, train_loss = train(\n",
        "            model, \n",
        "            X_train[i:i+BATCH_SIZE].float().to(device), \n",
        "            y_train[i:i+BATCH_SIZE].float().to(device), \n",
        "            criterion, \n",
        "            optimizer\n",
        "            )\n",
        "    train_losses.append(train_loss)\n",
        "        \n",
        "    val_output, val_loss = val(\n",
        "        model, \n",
        "        X_val.float().to(device), \n",
        "        y_val.float().to(device),\n",
        "        criterion\n",
        "        )\n",
        "    val_losses.append(val_loss)\n",
        "    val_output = (val_output > 0.5).float()\n",
        "    val_acc = accuracy_score(y_val, val_output.cpu().detach().numpy())\n",
        "    val_auc = roc_auc_score(y_val, val_output.cpu().detach().numpy())\n",
        "    val_aucs.append(val_auc)  \n",
        "\n",
        "    if val_losses[-1] > val_losses[-2] and val_losses[-2] > val_losses[-3]:\n",
        "        break   \n",
        "\n",
        "## Test accuracy and AUC\n",
        "test_output, test_loss = test(\n",
        "    model, \n",
        "    X_test.float().to(device), \n",
        "    y_test.float().to(device), \n",
        "    criterion\n",
        "    )\n",
        "test_output = (test_output > 0.5).float()\n",
        "test_acc = accuracy_score(y_test, test_output.cpu().detach().numpy())\n",
        "test_auc = roc_auc_score(y_test, test_output.cpu().detach().numpy())\n",
        "    \n",
        "best_auc = test_auc\n",
        "best_acc = test_acc\n",
        "best_output = test_output\n",
        "best_model = model\n",
        "\n",
        "## Confusion matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "print(\"Test accuracy: \", round(best_acc*100, 2), \"%\")\n",
        "print(\"Test AUC: \", round(best_auc, 2))\n",
        "print(\"Confusion Matrix: \")\n",
        "print(confusion_matrix(\n",
        "    y_test.cpu().detach().numpy(), \n",
        "    best_output.cpu().detach().numpy()\n",
        "    ))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Problematic Subgroups Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Create train, val, test split\n",
        "X = torch.cat((\n",
        "    torch.tensor(df_cm['predicted_action']),\n",
        "    torch.tensor(df_cm['predicted_object']),\n",
        "    torch.tensor(df_cm['predicted_location']),\n",
        "    torch.stack(\n",
        "        [torch.mean(torch.tensor(el[-1])) for el in df_cm['hidden_states']]\n",
        "        ).unsqueeze(1),\n",
        "    torch.tensor(df_cm['total_silence']).unsqueeze(1),\n",
        "    torch.tensor(df_cm['n_words']).unsqueeze(1),\n",
        "    torch.tensor(df_cm['speed_rate_word']).unsqueeze(1),\n",
        "    ), dim=1)\n",
        "X_train = X[:23132]\n",
        "X_val = X[23132:]\n",
        "\n",
        "X_test = torch.cat((\n",
        "    torch.tensor(df_cm_test['predicted_action']),\n",
        "    torch.tensor(df_cm_test['predicted_object']),\n",
        "    torch.tensor(df_cm_test['predicted_location']),\n",
        "    torch.stack(\n",
        "        [torch.mean(torch.tensor(el[-1])) for el in df_cm_test['hidden_states']]\n",
        "        ).unsqueeze(1),\n",
        "    torch.tensor(df_cm_test['total_silence']).unsqueeze(1),\n",
        "    torch.tensor(df_cm_test['n_words']).unsqueeze(1),\n",
        "    torch.tensor(df_cm_test['speed_rate_word']).unsqueeze(1),\n",
        "    ), dim=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Create train, val, test split\n",
        "y_subs = torch.tensor(df_discretized['subgID'])\n",
        "y_train_subs = y_subs[:23132]\n",
        "y_val_subs = y_subs[23132:]\n",
        "y_test_subs = torch.tensor(df_discretized_test['subgID'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "\n",
        "best_f1macro = 0\n",
        "best_acc = 0\n",
        "best_output = 0\n",
        "best_epoch = 0\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "test_losses = []\n",
        "\n",
        "if PRETRAIN:\n",
        "    best_model.linear3 = nn.Linear(\n",
        "        HIDDEN_SIZE,\n",
        "        NUM_SUBGROUPS+1\n",
        "        ).to(device)\n",
        "    model = best_model\n",
        "else:\n",
        "    model = ConfidenceModel(\n",
        "        input_size=X_train.shape[1],\n",
        "        hidden_size=HIDDEN_SIZE,\n",
        "        output_size=NUM_SUBGROUPS+1\n",
        "        ).to(device)\n",
        "\n",
        "## Criterion and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.NAdam(model.parameters(), lr=0.005)\n",
        "\n",
        "## Train and validate model\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    train_output, train_loss = train(\n",
        "        model,\n",
        "        X_train.to(device),\n",
        "        y_train_subs.to(device),\n",
        "        criterion,\n",
        "        optimizer\n",
        "        )\n",
        "    val_output, val_loss = val(\n",
        "        model,\n",
        "        X_val.to(device),\n",
        "        y_val_subs.to(device),\n",
        "        criterion\n",
        "        )\n",
        "\n",
        "    train_losses.append(train_loss)\n",
        "    val_losses.append(val_loss)\n",
        "\n",
        "    if val_loss > val_losses[-2] and val_loss > val_losses[-3]:\n",
        "        break\n",
        "\n",
        "test_output, test_loss = test(\n",
        "    model,\n",
        "    X_test.to(device),\n",
        "    y_test_subs.to(device),\n",
        "    criterion\n",
        "    )\n",
        "test_output = test_output.cpu().detach().numpy()\n",
        "test_output = np.argmax(test_output, axis=1)\n",
        "test_acc = accuracy_score(y_test_subs, test_output)\n",
        "test_f1 = f1_score(y_test_subs, test_output, average='macro')\n",
        "        \n",
        "best_f1macro = test_f1\n",
        "best_acc = test_acc\n",
        "best_output = test_outputå\n",
        "\n",
        "print(\"Test Accuracy: \", best_acc)\n",
        "print(\"Test F1 Macro: \", best_f1macro)\n",
        "print(\"--------------------\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Random Baseline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "y_subs = torch.tensor(df_discretized['subgID'])\n",
        "y_train_subs = y_subs[:23132]\n",
        "y_val_subs = y_subs[23132:]\n",
        "y_test_subs = torch.tensor(df_discretized_test['subgID'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Random baseline: assign random class to each sample\n",
        "\n",
        "SEED = 1\n",
        "np.random.seed(SEED)\n",
        "\n",
        "random_pred = np.random.randint(0, NUM_SUBGROUPS+1, len(y_test_subs))\n",
        "print(\"Test Accuracy: \", accuracy_score(y_test_subs, random_pred))\n",
        "print(\"F1 Macro: \", f1_score(y_test_subs, random_pred, average='macro'))\n",
        "\n",
        "## K = 2\n",
        "# Test Accuracy:    0.3311363037173741\n",
        "# F1 Macro:         0.21583683044821356\n",
        "\n",
        "## K = 3\n",
        "# Test Accuracy:    0.25046137621935144\n",
        "# F1 Macro:         0.1365154990425591\n",
        "\n",
        "## K = 4\n",
        "# Test Accuracy:    0.1982599525441603\n",
        "# F1 Macro:         0.11033841939589309\n",
        "\n",
        "## K = 5    \n",
        "# Test Accuracy:    0.17189559715264963\n",
        "# F1 Macro:         0.10379229291419767"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Random baseline: assign each sample to the most frequent class\n",
        "\n",
        "SEED = 1\n",
        "np.random.seed(SEED)\n",
        "\n",
        "# count number of samples for each class\n",
        "counts = np.zeros(NUM_SUBGROUPS+1)\n",
        "for i in range(NUM_SUBGROUPS+1):\n",
        "    counts[i] = len(y_test_subs[y_test_subs == i])\n",
        "\n",
        "most_frequent_pred = np.ones(len(y_test_subs))\n",
        "most_frequent_pred = most_frequent_pred * np.argmax(counts)\n",
        "print(\"Test Accuracy: \", accuracy_score(y_test_subs, most_frequent_pred))\n",
        "print(\"F1 Macro: \", f1_score(y_test_subs, most_frequent_pred, average='macro'))\n",
        "\n",
        "## K = 2\n",
        "# Test Accuracy:    0.9058792512523068\n",
        "# F1 Macro:         0.31687185871720386\n",
        "\n",
        "## K = 3\n",
        "# Test Accuracy:    0.8797785394147113\n",
        "# F1 Macro:         0.23401122019635343\n",
        "\n",
        "## K = 4\n",
        "# Test Accuracy:    0.8404956498813604\n",
        "# F1 Macro:         0.18266723965047987\n",
        "\n",
        "## K = 5\n",
        "# Test Accuracy:    0.7943580279462167\n",
        "# F1 Macro:         0.14756587324909393"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "SUPERB - IC Task (FSC).ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.4 ('amazon': conda)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "vscode": {
      "interpreter": {
        "hash": "a8dce71f01f4cf7d979b7741b7fb8d94cd1b30c77e0541871108952dcff484f0"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
