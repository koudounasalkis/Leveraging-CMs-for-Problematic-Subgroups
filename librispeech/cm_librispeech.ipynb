{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "cJVqXw3dRSX-"
      },
      "source": [
        "# Configuring the Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BTOyqWt3RXfU"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import librosa\n",
        "from datasets import load_dataset, Dataset\n",
        "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor, Wav2Vec2FeatureExtractor\n",
        "from jiwer import wer\n",
        "\n",
        "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, roc_auc_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np \n",
        "import pandas as pd\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import divexplorer \n",
        "import pandas as pd\n",
        "pd.set_option('max_colwidth', None)\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "from utils_analysis import filter_itemset_df_by_attributes, slice_by_itemset\n",
        "\n",
        "from divexplorer.FP_DivergenceExplorer import FP_DivergenceExplorer\n",
        "from divexplorer.FP_Divergence import FP_Divergence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FTbn0utIRbgS"
      },
      "outputs": [],
      "source": [
        "## Set device\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "SEED = 42\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "np.random.seed(SEED)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LibriSpeech Dataset - Inference and Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = Wav2Vec2ForCTC.from_pretrained(\n",
        "    \"wav2vec2-base-960h\", \n",
        "    output_hidden_states=True,\n",
        "    local_files_only=True\n",
        "    ).to(device)\n",
        "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
        "feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(\"facebook/wav2vec2-base-960h\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset_train = load_dataset(\"librispeech_asr\", \"clean\", split=\"train.360\")\n",
        "dataset_valid = load_dataset(\"librispeech_asr\", \"clean\", split=\"validation\")\n",
        "dataset_test = load_dataset(\"librispeech_asr\", \"clean\", split=\"test\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Train set\n",
        "last_hidden_states_concatenation_train = []\n",
        "avg_hidden_states_concatenation_train = []\n",
        "logits_concatenation_train = []\n",
        "sequence_lengths_train = []\n",
        "transcriptions_train = []\n",
        "wers_train = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for i in tqdm(range(0, len(dataset_train))):\n",
        "        input_values = feature_extractor(\n",
        "            dataset_train[i][\"audio\"][\"array\"], \n",
        "            return_tensors=\"pt\", \n",
        "            padding=\"longest\",\n",
        "            sampling_rate=feature_extractor.sampling_rate\n",
        "            ).input_values\n",
        "        outputs = model(input_values.to(device))\n",
        "        last_hidden_states_concatenation_train.append(outputs.hidden_states[-1])\n",
        "        logits_concatenation_train.append(outputs.logits)\n",
        "        sequence_lengths_train.append(outputs.logits.shape[1])\n",
        "\n",
        "        avg = [ hs.detach().cpu().numpy().squeeze() for hs in outputs.hidden_states ]\n",
        "        avg = [ np.mean(a, axis=1) for a in avg ]\n",
        "        avg_hidden_states_concatenation_train.append(np.mean(avg, axis=0))\n",
        "\n",
        "        predicted_ids = torch.argmax(outputs.logits, dim=-1)\n",
        "        transcription = processor.batch_decode(predicted_ids)\n",
        "        transcriptions_train.append(transcription[0])\n",
        "\n",
        "        wers_train.append(dataset_train[i][\"text\"], transcription[0])\n",
        "\n",
        "print(\"WER:\", wer(dataset_train[\"text\"], transcriptions_train))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Valid set\n",
        "last_hidden_states_concatenation_valid = []\n",
        "avg_hidden_states_concatenation_valid = []\n",
        "logits_concatenation_valid = []\n",
        "sequence_lengths_valid = []\n",
        "transcriptions_valid = []\n",
        "wers_valid = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for i in tqdm(range(0, len(dataset_valid))):\n",
        "        input_values = feature_extractor(\n",
        "            dataset_valid[i][\"audio\"][\"array\"], \n",
        "            return_tensors=\"pt\", \n",
        "            padding=\"longest\",\n",
        "            sampling_rate=feature_extractor.sampling_rate\n",
        "            ).input_values\n",
        "        outputs = model(input_values.to(device))\n",
        "        last_hidden_states_concatenation_valid.append(outputs.hidden_states[-1])\n",
        "        logits_concatenation_valid.append(outputs.logits)\n",
        "        sequence_lengths_valid.append(outputs.logits.shape[1])\n",
        "\n",
        "        avg = [ hs.detach().cpu().numpy().squeeze() for hs in outputs.hidden_states ]\n",
        "        avg = [ np.mean(a, axis=1) for a in avg ]\n",
        "        avg_hidden_states_concatenation_valid.append(np.mean(avg, axis=0))\n",
        "\n",
        "        predicted_ids = torch.argmax(outputs.logits, dim=-1)\n",
        "        transcription = processor.batch_decode(predicted_ids)\n",
        "        transcriptions_valid.append(transcription[0])\n",
        "\n",
        "        wers_valid.append(dataset_valid[i][\"text\"], transcription[0])\n",
        "\n",
        "print(\"WER:\", wer(dataset_valid[\"text\"], transcriptions_valid))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Test set  \n",
        "last_hidden_states_concatenation_test = []\n",
        "avg_hidden_states_concatenation_test = []\n",
        "logits_concatenation_test = []\n",
        "transcriptions_test = []\n",
        "sequence_lengths_test = []\n",
        "wers_test = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for i in tqdm(range(0, len(dataset_test))):\n",
        "        input_values = feature_extractor(\n",
        "            dataset_test[i][\"audio\"][\"array\"], \n",
        "            return_tensors=\"pt\", \n",
        "            padding=\"longest\",\n",
        "            sampling_rate=feature_extractor.sampling_rate\n",
        "            ).input_values\n",
        "        outputs = model(input_values.to(device))\n",
        "        last_hidden_states_concatenation_test.append(outputs.hidden_states[-1])\n",
        "        logits_concatenation_test.append(outputs.logits)\n",
        "        sequence_lengths_test.append(outputs.logits.shape[1])\n",
        "        \n",
        "        avg = [ hs.detach().cpu().numpy().squeeze() for hs in outputs.hidden_states ]\n",
        "        avg = [ np.mean(a, axis=1) for a in avg ]\n",
        "        avg_hidden_states_concatenation_valid.append(np.mean(avg, axis=0))\n",
        "\n",
        "        predicted_ids = torch.argmax(outputs.logits, dim=-1)\n",
        "        transcription = processor.batch_decode(predicted_ids)\n",
        "        transcriptions_test.append(transcription[0])\n",
        "\n",
        "        wers_test.append(dataset_test[i][\"text\"], transcription[0])\n",
        "\n",
        "print(\"WER:\", wer(dataset_test[\"text\"], transcriptions_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Save features\n",
        "torch.save(last_hidden_states_concatenation_train, \"pretrained/last_hidden_states_train.pt\")\n",
        "torch.save(avg_hidden_states_concatenation_train, \"pretrained/avg_hidden_states_train.pt\")\n",
        "torch.save(logits_concatenation_train, \"pretrained/logits_train.pt\")\n",
        "torch.save(sequence_lengths_train, \"pretrained/sequence_lengths_train.pt\")\n",
        "torch.save(transcriptions_train, \"pretrained/transcriptions_train.pt\")\n",
        "torch.save(wers_train, \"pretrained/wers_train.pt\")\n",
        "\n",
        "torch.save(last_hidden_states_concatenation_valid, \"pretrained/last_hidden_states_valid.pt\")\n",
        "torch.save(avg_hidden_states_concatenation_valid, \"pretrained/avg_hidden_states_valid.pt\")\n",
        "torch.save(logits_concatenation_valid, \"pretrained/logits_valid.pt\")\n",
        "torch.save(sequence_lengths_valid, \"pretrained/sequence_lengths_valid.pt\")\n",
        "torch.save(transcriptions_valid, \"pretrained/transcriptions_valid.pt\")\n",
        "\n",
        "torch.save(last_hidden_states_concatenation_test, \"pretrained/last_hidden_states_test.pt\")\n",
        "torch.save(avg_hidden_states_concatenation_test, \"pretrained/avg_hidden_states_test.pt\")\n",
        "torch.save(logits_concatenation_test, \"pretrained/logits_test.pt\")\n",
        "torch.save(sequence_lengths_test, \"pretrained/sequence_lengths_test.pt\")\n",
        "torch.save(transcriptions_test, \"pretrained/transcriptions_test.pt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Loading pretrained features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Load dataset\n",
        "dataset_train = load_dataset(\"librispeech_asr\", \"clean\", split=\"train.360\")\n",
        "dataset_valid = load_dataset(\"librispeech_asr\", \"clean\", split=\"validation\")\n",
        "dataset_test = load_dataset(\"librispeech_asr\", \"clean\", split=\"test\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Load hidden states and logits\n",
        "print(\"Loading train features...\")\n",
        "avg_hidden_states_train = torch.load('pretrained/avg_hidden_states_train.pt')\n",
        "last_hidden_states_train = torch.load('pretrained/last_hidden_states_train.pt')\n",
        "logits_concatenation_train = torch.load('pretrained/logits_concatenation_train.pt')\n",
        "sequence_lengths_train = torch.load('pretrained/sequence_lengths_train.pt')\n",
        "transcriptions_train = torch.load('pretrained/transcriptions_train.pt')\n",
        "wers_train = torch.load('pretrained/wers_train.pt')\n",
        "\n",
        "print(\"Loading valid features...\")\n",
        "avg_hidden_states_valid = torch.load('pretrained/avg_hidden_states_valid.pt')\n",
        "last_hidden_states_valid = torch.load('pretrained/last_hidden_states_valid.pt')\n",
        "logits_concatenation_valid = torch.load('pretrained/logits_concatenation_valid.pt')\n",
        "sequence_lengths_valid = torch.load('pretrained/sequence_lengths_valid.pt')\n",
        "transcriptions_valid = torch.load('pretrained/transcriptions_valid.pt')\n",
        "wers_valid = torch.load('pretrained/wers_valid.pt')\n",
        "\n",
        "print(\"Loading test features...\")\n",
        "avg_hidden_states_test = torch.load('pretrained/avg_hidden_states_test.pt')\n",
        "last_hidden_states_test = torch.load('pretrained/last_hidden_states_test.pt')\n",
        "logits_concatenation_test = torch.load('pretrained/logits_concatenation_test.pt')\n",
        "sequence_lengths_test = torch.load('pretrained/sequence_lengths_test.pt')\n",
        "transcriptions_test = torch.load('pretrained/transcriptions_test.pt')\n",
        "wers_test = torch.load('pretrained/wers_test.pt')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "P_SSWxldSmc-"
      },
      "source": [
        "# Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "prediction_train = (np.array(dataset_train[\"text\"]) == np.array(transcriptions_train)).astype(int)\n",
        "prediction_valid = (np.array(dataset_valid[\"text\"]) == np.array(transcriptions_valid)).astype(int)\n",
        "prediction_test = (np.array(dataset_test[\"text\"]) == np.array(transcriptions_test)).astype(int)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Confidence Model "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Confidence model\n",
        "class ConfidenceModel(nn.Module):\n",
        "    def __init__(self, input_size=768, hidden_size=500, output_size=1):\n",
        "        super(ConfidenceModel, self).__init__()\n",
        "        self.linear1 = nn.Linear(input_size, hidden_size)\n",
        "        self.linear2 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.linear3 = nn.Linear(hidden_size, output_size)\n",
        "        self.relu = nn.GELU()\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.norm = nn.LayerNorm(hidden_size)\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')\n",
        "                nn.init.zeros_(m.bias)\n",
        "                                     \n",
        "    def forward(self,x):\n",
        "        x = self.relu(self.linear1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.norm(x)\n",
        "        x = self.relu(self.linear2(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.norm(x)\n",
        "        x = self.sigmoid(self.linear3(x))\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Train, valid and test\n",
        "def train(model, inputs, labels, criterion, optimizer):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(inputs.float())\n",
        "    loss = criterion(outputs, labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return outputs, loss.item()\n",
        "\n",
        "def val(model, inputs, labels, criterion):\n",
        "    model.eval()\n",
        "    outputs = model(inputs.float())\n",
        "    loss = criterion(outputs, labels)\n",
        "    return outputs, loss.item()\n",
        "\n",
        "def test(model, inputs, labels=None, criterion=None):\n",
        "    model.eval()\n",
        "    if labels is None and criterion is None:\n",
        "        outputs = model(inputs.float())\n",
        "        return outputs\n",
        "    else:\n",
        "        outputs = model(inputs.float())\n",
        "        loss = criterion(outputs, labels)\n",
        "        return outputs, loss.item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torchsummary import summary\n",
        "\n",
        "model = ConfidenceModel(input_size=768, hidden_size=500, output_size=1)\n",
        "model = model.to(device)\n",
        "summary(model, input_size=(768,))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Problem Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "HIDDEN_SIZE = 500\n",
        "BATCH_SIZE = 4096\n",
        "NUM_SUBGROUPS = 2\n",
        "EPOCHS = 10000\n",
        "MIN_SUP = 0.05\n",
        "TH_REDUNDANDY = 0.001 \n",
        "PRETRAIN = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Problematic Subgroup Identification - DivExplorer, Step 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prepare df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "speakers = {}\n",
        "with open('SPEAKERS.TXT', 'r') as f:\n",
        "    lines = f.readlines()\n",
        "    for i,line in enumerate(lines):\n",
        "            speaker_id = line.strip().split(' ')[0]\n",
        "            if len(speaker_id) == 2:\n",
        "                gender = line.strip().split(' ')[4]\n",
        "            elif len(speaker_id) == 3:\n",
        "                gender = line.strip().split(' ')[3]\n",
        "            else:\n",
        "                gender = line.strip().split(' ')[2]\n",
        "            speakers[speaker_id] = gender\n",
        "\n",
        "gender_train = [speakers[str(sID)] for sID in dataset_train[\"speaker_id\"]]\n",
        "dataset_train = dataset_train.add_column(\"gender\", gender_train)\n",
        "\n",
        "gender_valid = [speakers[str(sID)] for sID in dataset_valid[\"speaker_id\"]]\n",
        "dataset_valid = dataset_valid.add_column(\"gender\", gender_valid) \n",
        "\n",
        "gender_test = [speakers[str(sID)] for sID in dataset_test[\"speaker_id\"]]\n",
        "dataset_test = dataset_test.add_column(\"gender\", gender_test) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_train = pd.read_csv('speech_metadata_train.csv')\n",
        "df_train['gender'] = gender_train\n",
        "df_train['prediction'] = prediction_train \n",
        "df_train['WER'] = wers_train\n",
        "\n",
        "df_valid = pd.read_csv('speech_metadata_valid.csv')\n",
        "df_valid['gender'] = gender_valid\n",
        "df_valid['prediction'] = prediction_valid \n",
        "df_valid['WER'] = wers_valid\n",
        "\n",
        "df_test = pd.read_csv('speech_metadata_test.csv')\n",
        "df_test['gender'] = gender_test\n",
        "df_test['prediction'] = prediction_test  \n",
        "df_test['WER'] = wers_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# concat df train and valid \n",
        "df_trainvalid = pd.concat([df_train, df_valid], ignore_index=True)\n",
        "len(df_trainvalid)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Define abbreviations for plot and visualization\n",
        "from divexplorer.FP_Divergence import abbreviateDict\n",
        "abbreviations = {\n",
        "    'total_silence': 'tot_silence', \\\n",
        "    'speaker_id' : 'spkID', \\\n",
        "    'trimmed': 'trim', \\\n",
        "    'total_':'tot_', \\\n",
        "    'speed_rate_word_trimmed': 'speakRate_trim', \\\n",
        "    'trim_duration': 'trim_dur', \\\n",
        "    'speed_rate_word':'speakRate', \\\n",
        "    'speed_rate_char':'speakCharRate', \\\n",
        "    'duration': 'dur'\n",
        "    }\n",
        "\n",
        "abbreviations_shorter = abbreviations.copy()\n",
        "\n",
        "## Function for sorting data cohorts\n",
        "def sortItemset(x, abbreviations={}):\n",
        "    x = list(x)\n",
        "    x.sort()\n",
        "    x = \", \".join(x)\n",
        "    for k, v in abbreviations.items():\n",
        "        x = x.replace(k, v)\n",
        "    return x\n",
        "\n",
        "def attributes_in_itemset(itemset, attributes, alls = True):\n",
        "    \"\"\" Check if attributes are in the itemset (all or at least one)\n",
        "    \n",
        "    Args:\n",
        "        itemset (frozenset): the itemset\n",
        "        attributes (list): list of itemset of interest\n",
        "        alls (bool): If True, check if ALL attributes of the itemset are the input attributes. \n",
        "        If False, check AT LEAST one attribute of the itemset is in the input attributes.\n",
        "        \n",
        "    \"\"\"\n",
        "    # Avoid returning the empty itemset (i.e., info of entire dataset)\n",
        "    if itemset == frozenset() and attributes:\n",
        "        return False\n",
        "    \n",
        "    for item in itemset:\n",
        "        # Get the attribute\n",
        "        attr_i = item.split(\"=\")[0]\n",
        "        \n",
        "        #If True, check if ALL attributes of the itemset are the input attributes.\n",
        "        if alls:\n",
        "            # Check if the attribute is present. If not, the itemset is not admitted\n",
        "            if attr_i not in attributes:\n",
        "                return False\n",
        "        else:\n",
        "            # Check if least one attribute. If yes, return True\n",
        "            if attr_i in attributes:\n",
        "                return True\n",
        "    if alls:\n",
        "        # All attributes of the itemset are indeed admitted\n",
        "        return True\n",
        "    else:\n",
        "        # Otherwise, it means that we find None\n",
        "        return False\n",
        "    \n",
        "def filter_itemset_df_by_attributes(df: pd.DataFrame, attributes: list, alls = True, itemset_col_name: str = \"itemsets\") -> pd.DataFrame:\n",
        "    \"\"\"Get the set of itemsets that have the attributes in the input list (all or at least one)\n",
        "    \n",
        "    Args:\n",
        "        df (pd.DataFrame): the input itemsets (with their info). \n",
        "        attributes (list): list of itemset of interest\n",
        "        alls (bool): If True, check if ALL attributes of the itemset are the input attributes. \n",
        "        If False, check AT LEAST one attribute of the itemset is in the input attributes.\n",
        "        itemset_col_name (str) : the name of the itemset column, \"itemsets\" as default\n",
        "        \n",
        "    Returns:\n",
        "        pd.DataFrame: the set of itemsets (with their info)\n",
        "    \"\"\"\n",
        "\n",
        "    return df.loc[df[itemset_col_name].apply(lambda x: attributes_in_itemset(x, attributes, alls = alls))]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ## Target for DivExplorer: \n",
        "# # 'prediction' is 1 if predicted_intet == original_intent, 0 otherwise\n",
        "# target_col = 'prediction' \n",
        "# target_metric = 'd_posr'\n",
        "# target_div = 'd_accuracy'\n",
        "# t_value_col = 't_value_tp_fn'\n",
        "\n",
        "## Target for DivExplorer: 'WER'\n",
        "target_col = 'WER' \n",
        "target_metric = 'd_outcome'\n",
        "target_div = f'd_{target_col}'\n",
        "t_value_col = 't_value_outcome'\n",
        "printable_columns = ['support', 'itemsets','WER', 'd_WER', 't_value']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ## Columns for visualization\n",
        "# show_cols = [\n",
        "#        'support', \n",
        "#        'itemsets', \n",
        "#        '#errors', \n",
        "#        '#corrects', \n",
        "#        'accuracy', \n",
        "#        'd_accuracy', \n",
        "#        't_value', \n",
        "#        'support_count', \n",
        "#        'length']\n",
        "# remapped_cols = {\n",
        "#        'tn': '#errors', \n",
        "#        'tp': '#corrects', \n",
        "#        'posr': 'accuracy', \n",
        "#        target_metric: target_div, \n",
        "#        't_value_tp_fn': 't_value'\n",
        "#        }\n",
        "\n",
        "## Columns for visualization\n",
        "remapped_cols = { \n",
        "       \"outcome\": target_col, \n",
        "       \"d_outcome\": target_div, \n",
        "       t_value_col: 't_value'}\n",
        "show_cols = [\n",
        "       'support', \n",
        "       'itemsets', \n",
        "       target_col, \n",
        "       target_div, \n",
        "       'support_count', \n",
        "       'length', \n",
        "       't_value'\n",
        "       ]\n",
        "\n",
        "## Columns of the df file that we are going to analyze \n",
        "demo_cols = ['gender']\n",
        "\n",
        "signal_cols = ['total_silence', 'total_duration', 'n_words', 'speed_rate_word']\n",
        "\n",
        "input_cols = demo_cols + signal_cols "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Discretize the dataframe\n",
        "from util_discretization import discretize\n",
        "\n",
        "df_discretized = discretize(\n",
        "    df_train[input_cols+[target_col]],\n",
        "    bins=3,\n",
        "    attributes=input_cols,\n",
        "    strategy=\"quantile\", \n",
        "    round_v = 2,\n",
        "    min_distinct=5,\n",
        ")\n",
        "\n",
        "## Replace values with ranges: \"low\", \"medium\", \"high\"\n",
        "replace_values = {}\n",
        "\n",
        "for i in range(0,len(signal_cols)):\n",
        "\n",
        "    for v in df_discretized[signal_cols[i]].unique():\n",
        "        if \"<=\" == v[0:2]:\n",
        "            replace_values[v] = \"low\"\n",
        "        elif \">\" == v[0]:\n",
        "            replace_values[v] = \"high\"\n",
        "        elif \"(\"  == v[0] and \"]\"  == v[-1]:\n",
        "            replace_values[v] = \"medium\"\n",
        "        else:\n",
        "            raise ValueError(v)\n",
        "\n",
        "    df_discretized[signal_cols[i]].replace(replace_values, inplace=True)\n",
        "\n",
        "## Create dict of Divergence df\n",
        "# fp_diver = FP_DivergenceExplorer(df_discretized, true_class_name=target_col, class_map={\"P\":1, \"N\":0})\n",
        "fp_diver = FP_DivergenceExplorer(df_discretized, target_name=target_col)\n",
        "FP_fm = fp_diver.getFrequentPatternDivergence(min_support=MIN_SUP, metrics=[target_metric])\n",
        "FP_fm.rename(columns=remapped_cols, inplace=True)\n",
        "FP_fm = FP_fm[show_cols].copy()\n",
        "# FP_fm['accuracy'] = round(FP_fm['accuracy'], 5)\n",
        "# FP_fm['d_accuracy'] = round(FP_fm['d_accuracy'], 5)\n",
        "FP_fm['WER'] = round(FP_fm['WER'], 5)\n",
        "FP_fm['d_WER'] = round(FP_fm['d_WER'], 5)\n",
        "FP_fm['t_value'] = round(FP_fm['t_value'], 2)\n",
        "fp_divergence = FP_Divergence(FP_fm, target_div)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Compute the divergence for Wav2Vec2-Base\n",
        "FPdiv = fp_divergence.getDivergence(th_redundancy=TH_REDUNDANCY)[::-1] \n",
        "\n",
        "## Retrieve Most Divergent Itemsets \n",
        "from copy import deepcopy\n",
        "pr = FPdiv.head(NUM_SUBGROUPS).copy()\n",
        "pr[\"support\"] = pr[\"support\"].round(2)\n",
        "pr[\"WER\"] = (pr[\"WER\"]*100).round(3)\n",
        "pr[\"d_WER\"] = (pr[\"d_WER\"]*100).round(3)\n",
        "# pr[\"#errors\"] = pr[\"#errors\"].astype(int)\n",
        "# pr[\"#corrects\"] = pr[\"#corrects\"].astype(int)\n",
        "# pr[\"accuracy\"] = (pr[\"accuracy\"]*100).round(3)\n",
        "# pr[\"d_accuracy\"] = (pr[\"d_accuracy\"]*100).round(3)\n",
        "display(pr)\n",
        "\n",
        "## Create a column in the df, and assign a class to each sample:\n",
        "# - 1 if the sample is in the most divergent itemset\n",
        "# - 2 if the sample is in the second most divergent itemset\n",
        "# - 3 if the sample is in the third most divergent itemset\n",
        "# - ...\n",
        "# - 0 otherwise\n",
        "df_discretized[\"subgID\"] = 0\n",
        "itemsets = []\n",
        "for i in range(NUM_SUBGROUPS):\n",
        "    itemsets.append(list(pr.itemsets.values[i]))\n",
        "for i in tqdm(range(0, len(df_discretized))):\n",
        "    for value,itemset in enumerate(itemsets):\n",
        "        ks = []\n",
        "        vs = []\n",
        "        for item in itemset:\n",
        "            k, v = item.split(\"=\")\n",
        "            ks.append(k)\n",
        "            vs.append(v)\n",
        "        if all(df_discretized.loc[i, ks] == vs):\n",
        "            if df_discretized.loc[i, \"subgID\"] == 0:\n",
        "                df_discretized.loc[i, \"subgID\"] = value+1\n",
        "            else:\n",
        "                continue\n",
        "        else:\n",
        "            continue\n",
        "\n",
        "for i in range(0,NUM_SUBGROUPS+1):\n",
        "    print(len(df_discretized.loc[df_discretized[\"subgID\"]==i]))\n",
        "    \n",
        "df_discretized.to_csv(\"df_discretized.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Valid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Discretize the dataframe\n",
        "from util_discretization import discretize\n",
        "\n",
        "df_discretized_valid = discretize(\n",
        "    df_valid[input_cols+[target_col]],\n",
        "    bins=3,\n",
        "    attributes=input_cols,\n",
        "    strategy=\"quantile\", \n",
        "    round_v = 2,\n",
        "    min_distinct=5,\n",
        ")\n",
        "\n",
        "## Replace values with ranges: \"low\", \"medium\", \"high\"\n",
        "replace_values = {}\n",
        "\n",
        "for i in range(0,len(signal_cols)):\n",
        "\n",
        "    for v in df_discretized_valid[signal_cols[i]].unique():\n",
        "        if \"<=\" == v[0:2]:\n",
        "            replace_values[v] = \"low\"\n",
        "        elif \">\" == v[0]:\n",
        "            replace_values[v] = \"high\"\n",
        "        elif \"(\"  == v[0] and \"]\"  == v[-1]:\n",
        "            replace_values[v] = \"medium\"\n",
        "        else:\n",
        "            raise ValueError(v)\n",
        "\n",
        "    df_discretized_valid[signal_cols[i]].replace(replace_values, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Create a column in the df, and assign a class to each sample:\n",
        "# - 1 if the sample is in the most divergent itemset\n",
        "# - 2 if the sample is in the second most divergent itemset\n",
        "# - 3 if the sample is in the third most divergent itemset\n",
        "# - ...\n",
        "# - 0 otherwise\n",
        "df_discretized_valid[\"subgID\"] = 0\n",
        "for i in tqdm(range(0, len(df_discretized_valid))):\n",
        "    for value,itemset in enumerate(itemsets):\n",
        "        ks = []\n",
        "        vs = []\n",
        "        for item in itemset:\n",
        "            k, v = item.split(\"=\")\n",
        "            ks.append(k)\n",
        "            vs.append(v)\n",
        "        if all(df_discretized_valid.loc[i, ks] == vs):\n",
        "            if df_discretized_valid.loc[i, \"subgID\"] == 0:\n",
        "                df_discretized_valid.loc[i, \"subgID\"] = value+1\n",
        "            else:\n",
        "                continue\n",
        "        else:\n",
        "            continue\n",
        "\n",
        "for i in range(0,NUM_SUBGROUPS+1):\n",
        "    print(len(df_discretized_valid.loc[df_discretized_valid[\"subgID\"]==i]))\n",
        "\n",
        "df_discretized_valid.to_csv(\"df_discretized_valid.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Discretize the dataframe\n",
        "from util_discretization import discretize\n",
        "\n",
        "df_discretized_test = discretize(\n",
        "    df_test[input_cols+[target_col]],\n",
        "    bins=3,\n",
        "    attributes=input_cols,\n",
        "    strategy=\"quantile\", \n",
        "    round_v = 2,\n",
        "    min_distinct=5,\n",
        ")\n",
        "\n",
        "## Replace values with ranges: \"low\", \"medium\", \"high\"\n",
        "replace_values = {}\n",
        "\n",
        "for i in range(0,len(signal_cols)):\n",
        "\n",
        "    for v in df_discretized_test[signal_cols[i]].unique():\n",
        "        if \"<=\" == v[0:2]:\n",
        "            replace_values[v] = \"low\"\n",
        "        elif \">\" == v[0]:\n",
        "            replace_values[v] = \"high\"\n",
        "        elif \"(\"  == v[0] and \"]\"  == v[-1]:\n",
        "            replace_values[v] = \"medium\"\n",
        "        else:\n",
        "            raise ValueError(v)\n",
        "\n",
        "    df_discretized_test[signal_cols[i]].replace(replace_values, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Create a column in the df, and assign a class to each sample:\n",
        "# - 1 if the sample is in the most divergent itemset\n",
        "# - 2 if the sample is in the second most divergent itemset\n",
        "# - 3 if the sample is in the third most divergent itemset\n",
        "# - ...\n",
        "# - 0 otherwise\n",
        "df_discretized_test[\"subgID\"] = 0\n",
        "for i in tqdm(range(0, len(df_discretized_test))):\n",
        "    for value,itemset in enumerate(itemsets):\n",
        "        ks = []\n",
        "        vs = []\n",
        "        for item in itemset:\n",
        "            k, v = item.split(\"=\")\n",
        "            ks.append(k)\n",
        "            vs.append(v)\n",
        "        if all(df_discretized_test.loc[i, ks] == vs):\n",
        "            if df_discretized_test.loc[i, \"subgID\"] == 0:\n",
        "                df_discretized_test.loc[i, \"subgID\"] = value+1\n",
        "            else:\n",
        "                continue\n",
        "        else:\n",
        "            continue\n",
        "\n",
        "for i in range(0,NUM_SUBGROUPS+1):\n",
        "    print(len(df_discretized_test.loc[df_discretized_test[\"subgID\"]==i]))\n",
        "    \n",
        "df_discretized_test.to_csv(\"df_discretized_test.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Full Pipeline - Steps 1 & 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_cm = df_trainvalid[[\n",
        "    'total_silence', 'n_words', 'speed_rate_word'\n",
        "    ]]\n",
        "df_cm_test = df_test[[\n",
        "    'total_silence', 'n_words', 'speed_rate_word'\n",
        "    ]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Pretraining the CM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train = torch.tensor(avg_hidden_states_train).squeeze()\n",
        "X_train = torch.mean(X_train, dim=1)\n",
        "X_train = torch.cat((\n",
        "    torch.tensor(logits_concatenation_train),\n",
        "    torch.tensor(sequence_lengths_train).unsqueeze(dim=1),\n",
        "    X_train,\n",
        "    torch.tensor(df_cm['total_silence'])[:len(dataset_train)].unsqueeze(1),\n",
        "    torch.tensor(df_cm['n_words'])[:len(dataset_train)].unsqueeze(1),\n",
        "    torch.tensor(df_cm['speed_rate_word'])[:len(dataset_train)].unsqueeze(1),\n",
        "    ), dim=1)\n",
        "y_train = torch.tensor(prediction_train).unsqueeze(1)\n",
        "\n",
        "X_val = torch.tensor(avg_hidden_states_valid).squeeze()\n",
        "X_val = torch.mean(X_val, dim=1)\n",
        "X_val = torch.cat((\n",
        "    torch.tensor(logits_concatenation_valid),\n",
        "    torch.tensor(sequence_lengths_valid).unsqueeze(dim=1),\n",
        "    X_val,\n",
        "    torch.tensor(df_cm['total_silence'])[len(dataset_train):].unsqueeze(1),\n",
        "    torch.tensor(df_cm['n_words'])[len(dataset_train):].unsqueeze(1),\n",
        "    torch.tensor(df_cm['speed_rate_word'])[len(dataset_train):].unsqueeze(1),\n",
        "    ), dim=1)\n",
        "y_val = torch.tensor(prediction_valid).unsqueeze(1)\n",
        "\n",
        "X_test = torch.tensor(avg_hidden_states_test).squeeze()\n",
        "X_test = torch.mean(X_test, dim=1)\n",
        "X_test = torch.cat((\n",
        "    torch.tensor(logits_concatenation_test),\n",
        "    torch.tensor(sequence_lengths_test).unsqueeze(dim=1),\n",
        "    X_test,\n",
        "    torch.tensor(df_cm_test['total_silence']).unsqueeze(1),\n",
        "    torch.tensor(df_cm_test['n_words']).unsqueeze(1),\n",
        "    torch.tensor(df_cm_test['speed_rate_word']).unsqueeze(1),\n",
        "    ), dim=1)\n",
        "y_test = torch.tensor(prediction_test).unsqueeze(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "\n",
        "best_auc = 0\n",
        "best_acc = 0\n",
        "best_output = 0\n",
        "best_model = 0\n",
        "\n",
        "if PRETRAIN:\n",
        "    ## Create model\n",
        "    model = ConfidenceModel(\n",
        "        input_size=X_train.shape[1],\n",
        "        hidden_size=HIDDEN_SIZE, \n",
        "        output_size=1\n",
        "        ).to(device)\n",
        "    criterion = nn.BCELoss()\n",
        "    optimizer = optim.NAdam(model.parameters(), lr=0.005) \n",
        "\n",
        "    ## Train model\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    val_aucs = []\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        \n",
        "        ## Train in batches\n",
        "        for i in range(0, len(X_train), BATCH_SIZE):\n",
        "            train_output, train_loss = train(\n",
        "                model, \n",
        "                X_train[i:i+BATCH_SIZE].float().to(device), \n",
        "                y_train[i:i+BATCH_SIZE].float().to(device), \n",
        "                criterion, \n",
        "                optimizer\n",
        "                )\n",
        "        train_losses.append(train_loss)\n",
        "            \n",
        "        val_output, val_loss = val(\n",
        "            model, \n",
        "            X_val.float().to(device), \n",
        "            y_val.float().to(device),\n",
        "            criterion\n",
        "            )\n",
        "        val_losses.append(val_loss)\n",
        "        val_output = (val_output > 0.5).float()\n",
        "        val_acc = accuracy_score(y_val, val_output.cpu().detach().numpy())\n",
        "        val_auc = roc_auc_score(y_val, val_output.cpu().detach().numpy())\n",
        "        val_aucs.append(val_auc)     \n",
        "\n",
        "        if val_losses[-1] > val_losses[-2] and val_losses[-2] > val_losses[-3]:\n",
        "            break\n",
        "\n",
        "    ## Test accuracy and AUC\n",
        "    test_output, test_loss = test(\n",
        "        model, \n",
        "        X_test.float().to(device), \n",
        "        y_test.float().to(device), \n",
        "        criterion\n",
        "        )\n",
        "    test_output = (test_output > 0.5).float()\n",
        "    test_acc = accuracy_score(y_test, test_output.cpu().detach().numpy())\n",
        "    test_auc = roc_auc_score(y_test, test_output.cpu().detach().numpy())\n",
        "    test_losses.append(test_loss)   \n",
        "    test_aucs.append(test_auc)         \n",
        "            \n",
        "    best_auc = test_auc\n",
        "    best_acc = test_acc\n",
        "    best_output = test_output\n",
        "    best_model = model\n",
        "\n",
        "    ## Confusion matrix\n",
        "    from sklearn.metrics import confusion_matrix\n",
        "    print(\"Test accuracy: \", round(best_acc*100, 2), \"%\")\n",
        "    print(\"Test AUC: \", round(best_auc, 2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Problematic Subgroups Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Create train, val, test split\n",
        "y_train_subs = torch.tensor(df_discretized['subgID'])\n",
        "y_val_subs = torch.tensor(df_discretized_valid['subgID'])\n",
        "y_test_subs = torch.tensor(df_discretized_test['subgID'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Train, valid and test\n",
        "def train(\n",
        "    model, \n",
        "    inputs, \n",
        "    labels, \n",
        "    criterion, \n",
        "    optimizer, \n",
        "    criterion1=None,\n",
        "    labels1=None,\n",
        "    alpha=0.5,\n",
        "    criterion2=None,\n",
        "    labels2=None,\n",
        "    ):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(inputs.float())\n",
        "    loss = criterion(outputs, labels)\n",
        "    if criterion1 is not None:\n",
        "        if labels1 is None:\n",
        "            loss1 = criterion1(outputs, labels)\n",
        "        else:\n",
        "            loss1 = criterion1(outputs, labels1.to(device))\n",
        "        loss = alpha * loss + (1-alpha) * loss1\n",
        "    if criterion2 is not None:\n",
        "        if labels2 is None:\n",
        "            loss2 = criterion2(outputs, labels).float()\n",
        "        else:\n",
        "            loss2 = criterion2(outputs, labels2.to(device))\n",
        "        loss = alpha * loss + (1-alpha) * loss2\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return outputs, loss.item()\n",
        "\n",
        "def val(\n",
        "    model, \n",
        "    inputs, \n",
        "    labels, \n",
        "    criterion, \n",
        "    criterion1=None, \n",
        "    labels1=None,\n",
        "    alpha=0.5,\n",
        "    criterion2=None,\n",
        "    labels2=None,\n",
        "    ):\n",
        "    model.eval()\n",
        "    outputs = model(inputs.float())\n",
        "    loss = criterion(outputs, labels)\n",
        "    if criterion1 is not None:\n",
        "        if labels1 is None:\n",
        "            loss1 = criterion1(outputs, labels)\n",
        "        else:\n",
        "            loss1 = criterion1(outputs, labels1.to(device))\n",
        "        loss = alpha * loss + (1-alpha) * loss1\n",
        "    if criterion2 is not None:\n",
        "        if labels2 is None:\n",
        "            loss2 = criterion2(outputs, labels)\n",
        "        else:\n",
        "            loss2 = criterion2(outputs, labels2.to(device))\n",
        "        loss = alpha * loss + (1-alpha) * loss2\n",
        "    return outputs, loss.item()\n",
        "\n",
        "def test(\n",
        "    model, \n",
        "    inputs, \n",
        "    labels=None, \n",
        "    criterion=None, \n",
        "    criterion1=None, \n",
        "    labels1=None,\n",
        "    alpha=0.5,\n",
        "    criterion2=None,\n",
        "    labels2=None,\n",
        "    ):\n",
        "    model.eval()\n",
        "    if labels is None and criterion is None:\n",
        "        outputs = model(inputs.float())\n",
        "        return outputs\n",
        "    else:\n",
        "        outputs = model(inputs.float())\n",
        "        loss = criterion(outputs, labels)\n",
        "        if criterion1 is not None:\n",
        "            if labels1 is None:\n",
        "                loss1 = criterion1(outputs, labels)\n",
        "            else:\n",
        "                loss1 = criterion1(outputs, labels1.to(device))\n",
        "            loss = alpha * loss + (1-alpha) * loss1\n",
        "        if criterion2 is not None:\n",
        "            if labels2 is None:\n",
        "                loss2 = criterion2(outputs, labels)\n",
        "            else:\n",
        "                loss2 = criterion2(outputs, labels2.to(device))\n",
        "            loss = alpha * loss + (1-alpha) * loss2\n",
        "        return outputs, loss.item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "criterion_loss = 'CE+MSE'\n",
        "ALPHA = 0.6\n",
        "\n",
        "y_train_subs = torch.tensor(df_discretized['subgID'])\n",
        "y_val_subs = torch.tensor(df_discretized_valid['subgID'])\n",
        "y_test_subs = torch.tensor(df_discretized_test['subgID'])\n",
        "\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "\n",
        "best_f1macro = 0\n",
        "best_acc = 0\n",
        "best_output = 0\n",
        "best_model_step3 = None\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "if PRETRAIN:\n",
        "    best_model.linear3 = nn.Linear(\n",
        "        HIDDEN_SIZE, \n",
        "        NUM_SUBGROUPS+1\n",
        "        ).to(device)\n",
        "    model = best_model\n",
        "else:\n",
        "    model = ConfidenceModel(\n",
        "        input_size=X_train.shape[1],\n",
        "        hidden_size=HIDDEN_SIZE, \n",
        "        output_size=NUM_SUBGROUPS+1\n",
        "        ).to(device)\n",
        "\n",
        "## Criterion and optimizer\n",
        "if criterion_loss == 'MSE' or criterion_loss == 'L1' or criterion_loss == 'MLML':\n",
        "    criterion = nn.MSELoss() if criterion_loss == 'MSE' else \\\n",
        "        (nn.L1Loss() if criterion_loss == 'L1' else nn.MultiLabelSoftMarginLoss())\n",
        "    X_train = X_train.float()\n",
        "    y_train_subs = y_train_subs.unsqueeze(dim=1).float()\n",
        "    X_val = X_val.float()\n",
        "    y_val_subs = y_val_subs.unsqueeze(dim=1).float()\n",
        "    X_test = X_test.float()\n",
        "    y_test_subs = y_test_subs.unsqueeze(dim=1).float()\n",
        "    criterion1 = None\n",
        "    criterion2 = None\n",
        "    y_train_subs_1 = None\n",
        "    y_val_subs_1 = None\n",
        "    y_test_subs_1 = None\n",
        "    y_train_subs_2 = None\n",
        "    y_val_subs_2 = None\n",
        "    y_test_subs_2 = None\n",
        "elif criterion_loss == 'MSE+L1' or criterion_loss == 'MSE+MLML':\n",
        "    criterion = nn.MSELoss()\n",
        "    criterion1 = nn.L1Loss() if criterion_loss == 'MSE+L1' else nn.MultiLabelSoftMarginLoss()\n",
        "    criterion2 = None\n",
        "    X_train = X_train.float()\n",
        "    y_train_subs = y_train_subs.unsqueeze(dim=1).float()\n",
        "    y_train_subs_1 = None\n",
        "    X_val = X_val.float()\n",
        "    y_val_subs = y_val_subs.unsqueeze(dim=1).float()\n",
        "    y_val_subs_1 = None\n",
        "    X_test = X_test.float()\n",
        "    y_test_subs = y_test_subs.unsqueeze(dim=1).float()\n",
        "    y_test_subs_1 = None\n",
        "    y_train_subs_2 = None\n",
        "    y_val_subs_2 = None\n",
        "    y_test_subs_2 = None\n",
        "elif criterion_loss == 'CE':\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    criterion1 = None\n",
        "    criterion2 = None\n",
        "    y_train_subs_1 = None\n",
        "    y_val_subs_1 = None\n",
        "    y_test_subs_1 = None\n",
        "    y_train_subs_2 = None\n",
        "    y_val_subs_2 = None\n",
        "    y_test_subs_2 = None\n",
        "elif criterion_loss == 'CE+MLML' or criterion_loss == 'CE+MSE' or criterion_loss == 'CE+L1':\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    criterion1 = nn.MultiLabelSoftMarginLoss() if criterion_loss == 'CE+MLML' else \\\n",
        "        (nn.MSELoss() if criterion_loss == 'CE+MSE' else nn.L1Loss())\n",
        "    y_train_subs_1 = y_train_subs.unsqueeze(dim=1).float().to(device)\n",
        "    y_val_subs_1 = y_val_subs.unsqueeze(dim=1).float().to(device)\n",
        "    y_test_subs_1 = y_test_subs.unsqueeze(dim=1).float().to(device)\n",
        "    y_train_subs_2 = None\n",
        "    y_val_subs_2 = None\n",
        "    y_test_subs_2 = None\n",
        "    criterion2 = None\n",
        "elif criterion_loss == 'CE+MSE+L1' or criterion_loss == 'CE+MSE+MLML':\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    criterion1 = nn.MSELoss() \n",
        "    criterion2 = nn.L1Loss() if criterion_loss == 'CE+MSE+L1' else nn.MultiLabelSoftMarginLoss()\n",
        "    y_train_subs_1 = y_train_subs.unsqueeze(dim=1).float().to(device)\n",
        "    y_val_subs_1 = y_val_subs.unsqueeze(dim=1).float().to(device)\n",
        "    y_test_subs_1 = y_test_subs.unsqueeze(dim=1).float().to(device)\n",
        "    y_train_subs_2 = y_train_subs.unsqueeze(dim=1).float().to(device)\n",
        "    y_val_subs_2 = y_val_subs.unsqueeze(dim=1).float().to(device)\n",
        "    y_test_subs_2 = y_test_subs.unsqueeze(dim=1).float().to(device)       \n",
        "elif criterion_loss == 'MLML+MSE+CE':\n",
        "    criterion = nn.MultiLabelSoftMarginLoss()\n",
        "    y_train_subs = y_train_subs.unsqueeze(dim=1).float().to(device)\n",
        "    y_val_subs = y_val_subs.unsqueeze(dim=1).float().to(device)\n",
        "    y_test_subs = y_test_subs.unsqueeze(dim=1).float().to(device)  \n",
        "    criterion1 = nn.MSELoss() \n",
        "    y_train_subs_1 = y_train_subs.unsqueeze(dim=1).float().to(device)\n",
        "    y_val_subs_1 = y_val_subs.unsqueeze(dim=1).float().to(device)\n",
        "    y_test_subs_1 = y_test_subs.unsqueeze(dim=1).float().to(device)\n",
        "    criterion2 = nn.CrossEntropyLoss\n",
        "    y_train_subs_2 = y_train_subs.to(device)\n",
        "    y_val_subs_2 = y_val_subs.to(device)\n",
        "    y_test_subs_2 = y_test_subs.to(device)  \n",
        "\n",
        "else:\n",
        "    print(\"Error: criterion_loss not valid\")\n",
        "    break\n",
        "\n",
        "optimizer = optim.NAdam(model.parameters(), lr=0.005)\n",
        "\n",
        "## Train and validate model\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "for epoch in tqdm(range(EPOCHS)):\n",
        "\n",
        "    train_output, train_loss = train(\n",
        "        model, \n",
        "        X_train.to(device), \n",
        "        y_train_subs.to(device), \n",
        "        criterion, \n",
        "        optimizer,\n",
        "        criterion1,\n",
        "        y_train_subs_1,\n",
        "        alpha=ALPHA,\n",
        "        criterion2=criterion2,\n",
        "        labels2=y_train_subs_2\n",
        "        )\n",
        "\n",
        "    val_output, val_loss = val(\n",
        "        model, \n",
        "        X_val.to(device), \n",
        "        y_val_subs.to(device), \n",
        "        criterion,\n",
        "        criterion1,\n",
        "        y_val_subs_1,\n",
        "        alpha=ALPHA,\n",
        "        criterion2=criterion2,\n",
        "        labels2=y_val_subs_2\n",
        "        )\n",
        "\n",
        "    train_losses.append(train_loss)\n",
        "    val_losses.append(val_loss)\n",
        "\n",
        "    if val_loss >= val_losses[-2] and val_loss >= val_losses[-3]:\n",
        "        break\n",
        "\n",
        "test_output, test_loss = test(\n",
        "    model, \n",
        "    X_test.to(device),\n",
        "    y_test_subs.to(device), \n",
        "    criterion,\n",
        "    criterion1,\n",
        "    y_test_subs_1,\n",
        "    alpha=ALPHA,\n",
        "    criterion2=criterion2,\n",
        "    labels2=y_test_subs_2\n",
        "    )\n",
        "test_output = test_output.cpu().detach().numpy()\n",
        "test_output = np.argmax(test_output, axis=1)\n",
        "test_acc = accuracy_score(y_test_subs, test_output)\n",
        "test_f1 = f1_score(y_test_subs, test_output, average='macro')\n",
        "\n",
        "best_f1macro = test_f1\n",
        "best_acc = test_acc\n",
        "best_output = test_output\n",
        "best_model_step3 = model\n",
        "\n",
        "print(\"Test Accuracy: \", best_acc)\n",
        "print(\"Test EER: \", 1-best_acc)\n",
        "print(\"Test F1 Macro: \", best_f1macro)\n",
        "print(\"--------------------\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Random baselines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Random baseline: assign random class to each sample\n",
        "\n",
        "SEED = 1\n",
        "np.random.seed(SEED)\n",
        "\n",
        "random_pred = np.random.randint(0, NUM_SUBGROUPS+1, len(y_test_subs))\n",
        "print(\"Test Accuracy: \", accuracy_score(y_test_subs, random_pred))\n",
        "print(\"F1 Macro: \", f1_score(y_test_subs, random_pred, average='macro'))\n",
        "\n",
        "## K = 2\n",
        "# Test Accuracy:    0.3316793893129771\n",
        "# F1 Macro:         0.3248792544071231\n",
        "\n",
        "## K = 3\n",
        "# Test Accuracy:    0.24923664122137404\n",
        "# F1 Macro:         0.2250682740747604\n",
        "\n",
        "## K = 4\n",
        "# Test Accuracy:    0.21259541984732824\n",
        "# F1 Macro:         0.16816526324538308\n",
        "\n",
        "## K = 5    \n",
        "# Test Accuracy:    0.17022900763358778\n",
        "# F1 Macro:         0.13941870409296506"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Random baseline: assign each sample to the most frequent class\n",
        "\n",
        "SEED = 1\n",
        "np.random.seed(SEED)\n",
        "\n",
        "# count number of samples for each class\n",
        "counts = np.zeros(NUM_SUBGROUPS+1)\n",
        "for i in range(NUM_SUBGROUPS+1):\n",
        "    counts[i] = len(y_test_subs[y_test_subs == i])\n",
        "\n",
        "most_frequent_pred = np.ones(len(y_test_subs))\n",
        "most_frequent_pred = most_frequent_pred * np.argmax(counts)\n",
        "print(\"Test Accuracy: \", accuracy_score(y_test_subs, most_frequent_pred))\n",
        "print(\"F1 Macro: \", f1_score(y_test_subs, most_frequent_pred, average='macro'))\n",
        "\n",
        "## K = 2\n",
        "# Test Accuracy:    0.44236641221374046\n",
        "# F1 Macro:         0.20446326188586048\n",
        "\n",
        "## K = 3\n",
        "# Test Accuracy:    0.40419847328244274\n",
        "# F1 Macro:         0.14392497961402556\n",
        "\n",
        "## K = 4\n",
        "# Test Accuracy:    0.40419847328244274\n",
        "# F1 Macro:         0.14392497961402556\n",
        "\n",
        "## K = 5\n",
        "# Test Accuracy:    0.33053435114503815\n",
        "# F1 Macro:         0.09936890418818131"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "SUPERB - IC Task (FSC).ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.4 ('amazon': conda)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "vscode": {
      "interpreter": {
        "hash": "a8dce71f01f4cf7d979b7741b7fb8d94cd1b30c77e0541871108952dcff484f0"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
